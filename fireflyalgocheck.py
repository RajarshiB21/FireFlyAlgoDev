# -*- coding: utf-8 -*-
"""FireFlyAlgoCheck.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1na05jHkMgOTnQ0ov1j7Av_9CY1z939TI
"""

#HeartDisease Prediction using Heart Data Features

"""**Heart Disease Prediction using independent medical features from Patients**

*Importing the Dependencies*
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

"""**Importing the Dataset from Files Section into a pandas Dataframe**"""

heart_data=pd.read_csv('/content/heart.csv')

heart_data.shape
#Displaying the number of rows and columns in the dataset

heart_data.head()
#Displaying the first 5 rows

heart_data.describe()
#Displaying the dataset in a statistical manner for better understanding

heart_data.isnull().sum()
#Finding out if there are any null values and if any how many

"""**Plotting :** Now we find the correlation between the variables and plot a heat map"""

#Plotting the HeatMap
correlation = heart_data.corr()
plt.figure(figsize=(10,10))
sns.heatmap(correlation, cbar=True, square=True, fmt='.1f', annot=True, annot_kws={'size':8}, cmap='Reds')

heart_data['target'].value_counts()
#Counting the number of positive and negative values with respect to the fact that whether or not the patient has a heart disease

#Splitting the dependent and independent variables into an X and Y variable 
x=heart_data.drop(columns='target',axis=1)
y=heart_data['target']
#Storing the target column values in the independent variable Y 
#Storing the rest of the variables after dropping the target column in X which in turn becomes the dependent variabes

#Printing the number of rows and columns of each variable 
print(x.shape)
print(y.shape)

#Printing the X and Y Variable to ensure that the data stroed in them is correct
print(x)
print(y)

"""**Splitting the data into Training and Test data** : 
we will be using stratify as the dataset is not large enough and there is a possibility that the y_test variable might only have 0's and only 1's.
Stratify helps us to split the data into training and test data with respect to the dependent variable Y
"""

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,stratify=y,random_state=0)

x_train.head()

x_test.head()

#Without stratify there is a possibility of uneven distribution of values in the Y variable : y_train, y_test
print(x.shape,x_train.shape,x_test.shape)

"""**Training and Building the Machine Learning Model**: For this experiment we will be using Logistic Regression since the dataset available to us is relatively small."""

model=LogisticRegression()
#As you can see we are currently going with only the default parameter. This will further be enhanced using hyperparameter tuning

#Model training and checking the prediction accuracy on the training data
model.fit(x_train,y_train)
train_pred=model.predict(x_train)
#Comparing the accuracy of the model by comparing the prediction of the model to the predefined labels in the dataset this will be done later down the line
print(train_pred)

accuracy1=accuracy_score(train_pred,y_train)
print(accuracy1)
#The Model gives us more than 80 percent accuracy on the training data as well

#prediction on the basis of the test data
test_pred=model.predict(x_test)
print(test_pred)

accuracy=accuracy_score(test_pred,y_test)
print(accuracy)
#The Model gives us more than 80 percent accuracy on the test data

"""**Making a predictive system**: This system is designed to take an input from the user where the user gives the model dependent variables and the model makes a prediction based on those variables to deliver a result"""

input_data=(56,1,1,120,240,0,1,169,0,0,0,0,2)
#This is where we will be taking the input
input_data_as_numpy=np.asarray(input_data)#Transforming the input data into a array for ease of access
input_data_reshaped=input_data_as_numpy.reshape(1,-1)#reshaping the data
prediction=model.predict(input_data_reshaped)
print(prediction)

#Using a simple if and else systen to display the output
if(prediction[0]==1):
  print('Patient has a Heart Condition and needs further medical examination')

else:
  print('Patient does not have a Heart Condition')

"""**Saving and Loading the trained model**"""

import pickle
#Pickle library helps us easily save and load trained machine learning models

filename='trained_model.sav'
#Coming up with a file name for our saved model
pickle.dump(model,open(filename,'wb'))
#Opening a file by the name of trained_model and we mention that it will be writing binary or wb

#Loading the saved model
loaded_model=pickle.load(open('trained_model.sav','rb'))
#rb stands for read binary

"""***For Firefly Algorithm*** - Our first step is to select the features with the greatest correlation with the dependent variable.
We will be using a copy of the main dataset, splitting into training and testing dataset and select the best possible features 

"""

heart_data.head()

df=heart_data#Copy of the heart_data dataset 
df1=heart_data#Copy of the dataset again

df.head()

df.info

#Checking if there are any null values that need filling 
df.isnull().sum()

#Let us view the statistical relationship one more time 
df.describe()

#Finally we view the shape of the dataset 
df.shape

"""**Feature Selection - Using Information Gain - Mutual Information in Classification Problem Statements**
MI Estimates mutual information for a discrete target variable

Mutual Information between two random variables is a non negative value which measures the dependency between the variables.

It is zero if and only if two random varialbes are independent. Higher the values means higher dependency on the target variable 

The function relies on non parametric methods based on entropy estimation from k-nearest-neighbors distance

In short, it is a quantity that measures the amount of information one can obtain from one random variable given another random variable

It is represented as:
**I(X:Y)=H(X)-H(X|Y)**
where I(X:Y) is the mutual information for X and Y, H(X) is the entropy for X and H(X|Y) is the conditional entropy for X given Y varaiable. The result is always in the form of bits
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
#First we will test the accuracy without feature selction using a classifier model
from sklearn.tree import DecisionTreeClassifier

#Making a copy of the dataset again
df3=heart_data#For training the model without feature selection
df4=heart_data#Used for Feature Selection
df5=heart_data#Used by the model for accuracy testing after feature selection
X3=df3.drop(columns='target',axis=1)#Dropping the target column
Y3=df3['target']#Keeping the target column

#Train Test Splitting
X3_train,x3_test,y3_train,y3_test=train_test_split(X3,Y3,test_size=0.2,random_state=0)

#We will be using Decision Tree Classifier for this Classification problem
model1=DecisionTreeClassifier()

#Here we create,train and get the model to predict on the test dataset to determine the accuracy 
model1.fit(X3_train,y3_train)
train_pred1=model1.predict(X3_train)
test_pred1=model1.predict(x3_test)
accuracy1=accuracy_score(test_pred1,y3_test)
accuracyy1=accuracy_score(model1.predict(X3_train),y3_train)
print("Accuracy on the testing dataset for Decision Tree Classifier is:",accuracy1)

X4=df4.drop(columns=['target'],axis=1)
Y4=df4['target']

X4.head()

#Feature Selection
X4_train,x4_test,y4_train,y4_test=train_test_split(X4,Y4,test_size=0.2,random_state=0)

from sklearn.feature_selection import mutual_info_classif
mutual_info=mutual_info_classif(X4_train,y4_train)
mutual_info

mutual_info=pd.Series(mutual_info)
mutual_info.index=X4_train.columns
mutual_info.sort_values(ascending=False)

mutual_info.sort_values(ascending=False).plot.bar(figsize=(20,8))

from sklearn.feature_selection import SelectKBest
sel_five_cols=SelectKBest(mutual_info_classif,k=3)
sel_five_cols.fit(X4_train,y4_train)
X4_train.columns[sel_five_cols.get_support()]

X5=df5[{'cp', 'exang', 'thal'}]
Y5=df5['target']

print(X5.shape,Y5.shape)

#Feature Selection
X5_train,x5_test,y5_train,y5_test=train_test_split(X5,Y5,test_size=0.2,random_state=0)

model2=DecisionTreeClassifier()

model2.fit(X5_train,y5_train)
train_pred2=model2.predict(X5_train)
test_pred2=model2.predict(x5_test)
accuracy2=accuracy_score(test_pred2,y5_test)
print("Accuracy on the testing dataset for Decision Tree Classifier is:",accuracy2)

"""After Feature Selection we can see that the accuracy has increased from 77 percent to 80 percent on the testing dataset """

X5.head()

def calc_chi(data):
  mean=data.mean()
  for i in range(len(data)):
    diff=(data[i]-mean)**2
    return diff/mean

column_names=X5.columns
chi_val=[]
for i in range (len(column_names)):
  col_name=column_names[i]
  chi_val.append(calc_chi(X5[col_name]))

print(chi_val)#Chi Value for three features

#Calculating the intensity
intensity=[]
def sphere(x):
  for i in range (len(x)):
    sq=x[i]**2
    intensity.append(sq)
  return intensity
sphere(chi_val)
print(intensity)

def sphere2(x):
    return np.sum(x ** 2)

chi_val1=chi_val
intensity1=intensity
new_data=pd.DataFrame(chi_val,columns=["Chi value"])
new_data["Intensity"]=intensity
new_data.head()

import random
npop=len(chi_val)
t=10000
betamin=1
gamma=0.1
evaluation=npop
ub=5
lb=-5
search_range = ub - lb

def randnum():
  return random.random()

alpha=1.0
new_alpha = alpha

while(evaluation<t):
    npop=len(chi_val)
    t=10000
    betamin=1
    gamma=0.1
    evaluation=npop
    ub=5
    lb=-5
    search_range = ub - lb  
    new_alpha *= 0.97
    for i in range(npop):
      for j in range(i):
        if(intensity1[i]<intensity[j]):
          r=chi_val1[i]-chi_val1[j]
          beta = betamin * np.exp(-gamma * r)
          steps = new_alpha * (randnum() - 0.5) * search_range
          chi_val[i] += beta * (chi_val[j] - chi_val[i]) + steps
          chi_val[i] = np.clip(chi_val[i], lb, ub)
          new_intensity= sphere2(chi_val[i])
          intensity[i] =new_intensity
          evaluation += 1
          best = min(intensity[i], best)
return best

